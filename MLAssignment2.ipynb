{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1XWK1LyfCW0tWvLdH87CVuqUVXleuaX1v",
      "authorship_tag": "ABX9TyNWm7e7BR9MnJGnQJZpytOL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anupamamnair/anupamamnair.github.io/blob/master/MLAssignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a Support Vector Machine (SVM) classifier in Python to predict passenger survival on the Titanic dataset. The target variable is Survived (0 = did not survive, 1 = survived)."
      ],
      "metadata": {
        "id": "E9hG2BbqkKeq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LvRf-to9Fum",
        "outputId": "fb0354a9-2f04-415a-9e32-7d59007c0799",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial data shape: (891, 12)\n",
            "   PassengerId  Survived  Pclass  \\\n",
            "0            1         0       3   \n",
            "1            2         1       1   \n",
            "2            3         1       3   \n",
            "3            4         1       1   \n",
            "4            5         0       3   \n",
            "\n",
            "                                                Name     Sex   Age  SibSp  \\\n",
            "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
            "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
            "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
            "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
            "4                           Allen, Mr. William Henry    male  35.0      0   \n",
            "\n",
            "   Parch            Ticket     Fare Cabin Embarked  \n",
            "0      0         A/5 21171   7.2500   NaN        S  \n",
            "1      0          PC 17599  71.2833   C85        C  \n",
            "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
            "3      0            113803  53.1000  C123        S  \n",
            "4      0            373450   8.0500   NaN        S  \n",
            "Missing values\n",
            "Age         177\n",
            "Embarked      2\n",
            "dtype: int64\n",
            "\n",
            "======= Training SVM with LINEAR kernel =======\n",
            "\n",
            "Classification Report for LINEAR Kernel:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.84      0.82       105\n",
            "           1       0.75      0.70      0.73        74\n",
            "\n",
            "    accuracy                           0.78       179\n",
            "   macro avg       0.78      0.77      0.77       179\n",
            "weighted avg       0.78      0.78      0.78       179\n",
            "\n",
            "\n",
            "======= Training SVM with POLY kernel =======\n",
            "\n",
            "Classification Report for POLY Kernel:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.90      0.84       105\n",
            "           1       0.82      0.66      0.73        74\n",
            "\n",
            "    accuracy                           0.80       179\n",
            "   macro avg       0.80      0.78      0.79       179\n",
            "weighted avg       0.80      0.80      0.79       179\n",
            "\n",
            "\n",
            "======= Training SVM with RBF kernel =======\n",
            "\n",
            "Classification Report for RBF Kernel:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.90      0.86       105\n",
            "           1       0.84      0.70      0.76        74\n",
            "\n",
            "    accuracy                           0.82       179\n",
            "   macro avg       0.83      0.80      0.81       179\n",
            "weighted avg       0.82      0.82      0.82       179\n",
            "\n",
            "\n",
            "===== Model Performance Comparison =====\n",
            "        Accuracy  Precision    Recall  F1-score\n",
            "linear  0.782123   0.753623  0.702703  0.727273\n",
            "poly    0.798883   0.816667  0.662162  0.731343\n",
            "rbf     0.821229   0.838710  0.702703  0.764706\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/anupamamnair/iiitk/main/titanic.csv\")\n",
        "print(\"Initial data shape:\", df.shape)\n",
        "print(df.head())\n",
        "\n",
        "# Data Preprocessing\n",
        "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\n",
        "target = 'Survived'\n",
        "df = df[features + [target]]\n",
        "\n",
        "print(\"Missing values\")\n",
        "print(df[df.columns[df.isnull().any()]].isnull().sum())\n",
        "\n",
        "# Handle missing values\n",
        "df['Age'] = df['Age'].fillna(df['Age'].median())\n",
        "df['Embarked'] = df['Embarked'].fillna(df['Embarked'].mode()[0])\n",
        "\n",
        "# Encode categorical variables\n",
        "df = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True)\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(target, axis=1)\n",
        "y = df[target]\n",
        "\n",
        "# Splitting data to train and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train SVM models with different kernels\n",
        "kernels = ['linear', 'poly', 'rbf']\n",
        "results = {}\n",
        "\n",
        "for kernel in kernels:\n",
        "    print(f\"\\n======= Training SVM with {kernel.upper()} kernel =======\")\n",
        "    svm = SVC(kernel=kernel, random_state=42)\n",
        "    svm.fit(X_train, y_train)\n",
        "    y_pred = svm.predict(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred)\n",
        "    rec = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    results[kernel] = {\n",
        "        'Accuracy': acc,\n",
        "        'Precision': prec,\n",
        "        'Recall': rec,\n",
        "        'F1-score': f1\n",
        "    }\n",
        "\n",
        "    print(f\"\\nClassification Report for {kernel.upper()} Kernel:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\n===== Model Performance Comparison =====\")\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a Support Vector Machine (SVM) classifier in Python to predict whether a person earns more than $50K per year using the Adult Income dataset (also known as Census Income dataset)."
      ],
      "metadata": {
        "id": "CMNDZHtGkIqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/anupamamnair/iiitk/main/adult.csv\")\n",
        "print(\"Initial data shape:\", df.shape)\n",
        "print(df.head())\n",
        "\n",
        "# Data Preprocessing\n",
        "# Replace '?' with NaN and drop missing rows\n",
        "df = df.replace('?', np.nan).dropna()\n",
        "\n",
        "# Encode categorical variables\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop('income_>50K', axis=1)\n",
        "y = df['income_>50K']\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Train SVM models with different kernels\n",
        "kernels = ['linear', 'poly', 'rbf']\n",
        "results = {}\n",
        "\n",
        "for kernel in kernels:\n",
        "    print(f\"\\n======= Training SVM with {kernel.upper()} kernel =======\")\n",
        "    svm = SVC(kernel=kernel, random_state=42)\n",
        "    svm.fit(X_train, y_train)\n",
        "    y_pred = svm.predict(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    prec = precision_score(y_test, y_pred)\n",
        "    rec = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    results[kernel] = {\n",
        "        'Accuracy': acc,\n",
        "        'Precision': prec,\n",
        "        'Recall': rec,\n",
        "        'F1-score': f1\n",
        "    }\n",
        "\n",
        "    print(f\"\\nClassification Report for {kernel.upper()} Kernel:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(\"\\n===== Model Performance Comparison =====\")\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9HwR0jm-k43",
        "outputId": "09432cdc-882f-4265-de34-5150062416b1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial data shape: (32561, 15)\n",
            "   age workclass  fnlwgt     education  education.num marital.status  \\\n",
            "0   90         ?   77053       HS-grad              9        Widowed   \n",
            "1   82   Private  132870       HS-grad              9        Widowed   \n",
            "2   66         ?  186061  Some-college             10        Widowed   \n",
            "3   54   Private  140359       7th-8th              4       Divorced   \n",
            "4   41   Private  264663  Some-college             10      Separated   \n",
            "\n",
            "          occupation   relationship   race     sex  capital.gain  \\\n",
            "0                  ?  Not-in-family  White  Female             0   \n",
            "1    Exec-managerial  Not-in-family  White  Female             0   \n",
            "2                  ?      Unmarried  Black  Female             0   \n",
            "3  Machine-op-inspct      Unmarried  White  Female             0   \n",
            "4     Prof-specialty      Own-child  White  Female             0   \n",
            "\n",
            "   capital.loss  hours.per.week native.country income  \n",
            "0          4356              40  United-States  <=50K  \n",
            "1          4356              18  United-States  <=50K  \n",
            "2          4356              40  United-States  <=50K  \n",
            "3          3900              40  United-States  <=50K  \n",
            "4          3900              40  United-States  <=50K  \n",
            "\n",
            "======= Training SVM with LINEAR kernel =======\n",
            "\n",
            "Classification Report for LINEAR Kernel:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.87      0.93      0.90      4533\n",
            "        True       0.72      0.58      0.64      1500\n",
            "\n",
            "    accuracy                           0.84      6033\n",
            "   macro avg       0.80      0.75      0.77      6033\n",
            "weighted avg       0.83      0.84      0.83      6033\n",
            "\n",
            "\n",
            "======= Training SVM with POLY kernel =======\n",
            "\n",
            "Classification Report for POLY Kernel:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.84      0.94      0.89      4533\n",
            "        True       0.74      0.47      0.57      1500\n",
            "\n",
            "    accuracy                           0.83      6033\n",
            "   macro avg       0.79      0.71      0.73      6033\n",
            "weighted avg       0.82      0.83      0.81      6033\n",
            "\n",
            "\n",
            "======= Training SVM with RBF kernel =======\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a Support Vector Machine (SVM) classifier in Python to predict whether a person is likely to suffer a stroke using the Stroke Prediction dataset. The target variable is stroke (0 = No, 1 = Yes)."
      ],
      "metadata": {
        "id": "z1dJY7SqkbP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/anupamamnair/iiitk/main/healthcare-dataset-stroke-data.csv')\n",
        "print(\"Initial data shape:\", df.shape)\n",
        "print(df.head())\n",
        "\n",
        "# Data Preprocessing\n",
        "# Drop irrelevant columns\n",
        "df = df.drop(columns=['id'])\n",
        "\n",
        "# Preprocess the data\n",
        "# Handle missing values\n",
        "df['bmi'] = df['bmi'].fillna(df['bmi'].median())\n",
        "\n",
        "# Encode categorical variables\n",
        "df = pd.get_dummies(df, columns=['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status'], drop_first=True)\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop('stroke', axis=1)\n",
        "y = df['stroke']\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "num_features = ['age', 'hypertension', 'heart_disease', 'avg_glucose_level', 'bmi']\n",
        "X[num_features] = scaler.fit_transform(X[num_features])\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Train SVM classifier with different kernels\n",
        "kernels = ['linear', 'rbf', 'poly']\n",
        "results = {}\n",
        "\n",
        "for kernel in kernels:\n",
        "    clf = SVC(kernel=kernel, probability=True, class_weight='balanced', random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    y_prob = clf.predict_proba(X_test)[:,1]\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    cr = classification_report(y_test, y_pred, zero_division=0)\n",
        "    roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "    results[kernel] = {'accuracy': acc, 'confusion_matrix': cm, 'classification_report': cr, 'roc_auc': roc_auc}\n",
        "\n",
        "for k, v in results.items():\n",
        "    print(f\"\\n======= SVM with {k} kernel =======\")\n",
        "    print(\"Accuracy:\", v['accuracy'])\n",
        "    print(\"Confusion Matrix:\\n\", v['confusion_matrix'])\n",
        "    print(\"Classification Report:\\n\", v['classification_report'])\n",
        "    print(\"ROC-AUC Score:\", v['roc_auc'])\n",
        "\n",
        "#Plot ROC curves\n",
        "plt.figure(figsize=(8,6))\n",
        "for kernel in kernels:\n",
        "    clf = SVC(kernel=kernel, probability=True, random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_prob = clf.predict_proba(X_test)[:,1]\n",
        "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "    plt.plot(fpr, tpr, label=f'{kernel} (AUC = {roc_auc_score(y_test, y_prob):.2f})')\n",
        "\n",
        "plt.plot([0,1], [0,1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curves for SVM Kernels')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NzCVSly4ekBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a Naive Bayes classifier in Python to predict loan approval using the Loan Prediction dataset. The target variable is Loan_Status."
      ],
      "metadata": {
        "id": "An85-kaUkif5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/anupamamnair/iiitk/main/loan_status.csv')\n",
        "print(\"Initial data shape:\", df.shape)\n",
        "print(df.head())\n",
        "\n",
        "# Data Preprocessing\n",
        "# Drop irrelevant columns\n",
        "df.drop(columns=['Loan_ID'], inplace=True)\n",
        "\n",
        "# Handle missing values\n",
        "imputer = SimpleImputer(strategy='most_frequent')\n",
        "df = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
        "\n",
        "# Encode categorical variables\n",
        "label_encoder = LabelEncoder()\n",
        "categorical_columns = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area', 'Loan_Status']\n",
        "for col in categorical_columns:\n",
        "    df[col] = label_encoder.fit_transform(df[col])\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop('Loan_Status', axis=1)\n",
        "y = df['Loan_Status']\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "X[['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History']] = scaler.fit_transform(\n",
        "    X[['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History']])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train Naive Bayes classifier\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_nb = nb_model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"========= Naive Bayes Classifier ========= \")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_nb))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_nb))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_nb))\n",
        "\n",
        "\n",
        "# Initialize and train SVM classifier\n",
        "svm_model = SVC(kernel='linear', random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"========= SVM Classifier =========\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_svm))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_svm))\n",
        "\n"
      ],
      "metadata": {
        "id": "rXLcxqzJfeXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison\n",
        "SVM classifier with a linear kernel performs slightly better in this case, particularly in identifying approved loans. However, both models have limitations in correctly identifying loans that are not approved."
      ],
      "metadata": {
        "id": "Trewd8ghLTmd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a Naive Bayes classifier in Python to predict whether a customer will subscribe to a term deposit based on demographic, financial, and campaign-related attributes using the Bank Marketing dataset"
      ],
      "metadata": {
        "id": "grNZ8LUFkoUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/anupamamnair/iiitk/main/bank.csv')\n",
        "print(\"Initial data shape:\", df.shape)\n",
        "print(df.head())\n",
        "\n",
        "target_col = 'deposit'\n",
        "\n",
        "# Data Preprocessing\n",
        "# Encode categorical variables\n",
        "cat_cols = df.select_dtypes(include='object').columns\n",
        "le = LabelEncoder()\n",
        "for col in cat_cols:\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "\n",
        "# Encode target\n",
        "df[target_col] = le.fit_transform(df[target_col])\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(target_col, axis=1)\n",
        "y = df[target_col]\n",
        "\n",
        "# Normalize numerical features if required\n",
        "num_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "scaler = StandardScaler()\n",
        "X[num_cols] = scaler.fit_transform(X[num_cols])\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Train Naive Bayes classifier\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "y_pred_nb = nb_model.predict(X_test)\n",
        "y_prob_nb = nb_model.predict_proba(X_test)[:,1]\n",
        "\n",
        "# Train Logistic Regression\n",
        "lr_model = LogisticRegression(max_iter=1000)\n",
        "lr_model.fit(X_train, y_train)\n",
        "y_pred_lr = lr_model.predict(X_test)\n",
        "y_prob_lr = lr_model.predict_proba(X_test)[:,1]\n",
        "\n",
        "# Function to evaluate the model and print metrics\n",
        "def evaluate_model(name, y_true, y_pred, y_prob):\n",
        "    print(f\"\\n============ {name} ============\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
        "    print(\"Classification Report:\\n\", classification_report(y_true, y_pred, zero_division=0))\n",
        "    print(\"ROC-AUC:\", roc_auc_score(y_true, y_prob))\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Evaluate models\n",
        "evaluate_model(\"Naive Bayes Classifier\", y_test, y_pred_nb, y_prob_nb)\n",
        "evaluate_model(\"Logistic Regression Classifier\", y_test, y_pred_lr, y_prob_lr)\n",
        "\n",
        "# Plot ROC curves\n",
        "plt.figure(figsize=(8,6))\n",
        "fpr_nb, tpr_nb, _ = roc_curve(y_test, y_prob_nb)\n",
        "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_prob_lr)\n",
        "\n",
        "plt.plot(fpr_nb, tpr_nb, label=f'Naive Bayes (AUC = {roc_auc_score(y_test, y_prob_nb):.2f})')\n",
        "plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {roc_auc_score(y_test, y_prob_lr):.2f})')\n",
        "plt.plot([0,1], [0,1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curves')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "AALySP7CgYm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison:\n",
        "\n",
        "The Logistic Regression model generally outperforms the Naive Bayes model on this dataset.\n",
        "Logistic Regression has higher accuracy, a better balance between precision and recall for both classes (especially the 'Yes Subscription' class), and a higher ROC-AUC score.\n",
        "While Naive Bayes has a slightly higher recall for the 'Yes Subscription' class, Logistic Regression has better precision for this class and a higher recall for the 'No Subscription' class."
      ],
      "metadata": {
        "id": "DYURKze5K_ay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a Naive Bayes classifier in Python to predict whether an employee will leave the company based on job satisfaction, work environment, and personal attributes. The target variable is Attrition (Yes / No)."
      ],
      "metadata": {
        "id": "zkTI44s4kv95"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/anupamamnair/iiitk/main/empAttrition.csv')\n",
        "print(\"Initial data shape:\", df.shape)\n",
        "print(df.head())\n",
        "\n",
        "# Data Preprocessing\n",
        "target_col = 'Attrition'\n",
        "\n",
        "# Encode categorical variables\n",
        "cat_cols = df.select_dtypes(include='object').columns\n",
        "cat_cols = [col for col in cat_cols if col != target_col]\n",
        "\n",
        "le = LabelEncoder()\n",
        "for col in cat_cols:\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "\n",
        "# Encode target\n",
        "df[target_col] = le.fit_transform(df[target_col])\n",
        "\n",
        "# Separate features and target\n",
        "X = df.drop(target_col, axis=1)\n",
        "y = df[target_col]\n",
        "\n",
        "# Normalize numerical features\n",
        "num_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "scaler = StandardScaler()\n",
        "X[num_cols] = scaler.fit_transform(X[num_cols])\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Train Naive Bayes\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "y_pred_nb = nb_model.predict(X_test)\n",
        "y_prob_nb = nb_model.predict_proba(X_test)[:,1]\n",
        "\n",
        "# Train Random Forest\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "y_prob_rf = rf_model.predict_proba(X_test)[:,1]\n",
        "\n",
        "def evaluate_model(name, y_true, y_pred, y_prob):\n",
        "    print(f\"=== {name} ===\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
        "    print(\"Classification Report:\\n\", classification_report(y_true, y_pred, zero_division=0))\n",
        "    print(\"ROC-AUC Score:\", roc_auc_score(y_true, y_prob))\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Evaluate models\n",
        "evaluate_model(\"Naive Bayes\", y_test, y_pred_nb, y_prob_nb)\n",
        "evaluate_model(\"Random Forest\", y_test, y_pred_rf, y_prob_rf)\n",
        "\n",
        "# Plot ROC curves\n",
        "plt.figure(figsize=(8,6))\n",
        "fpr_nb, tpr_nb, _ = roc_curve(y_test, y_prob_nb)\n",
        "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_prob_rf)\n",
        "\n",
        "plt.plot(fpr_nb, tpr_nb, label=f'Naive Bayes (AUC = {roc_auc_score(y_test, y_prob_nb):.2f})')\n",
        "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {roc_auc_score(y_test, y_prob_rf):.2f})')\n",
        "plt.plot([0,1], [0,1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curves')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vtB23WnhjfOO",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison:\n",
        "\n",
        "The Random Forest model has a higher overall accuracy and a better ROC-AUC score, indicating it is generally better at distinguishing between the two classes.\n",
        "However, the Random Forest model has a significantly lower recall for the 'Yes Attrition' class (0.11 vs 0.66 for Naive Bayes). This means it misses a large proportion of employees who actually leave.\n",
        "The Naive Bayes model has a higher recall for the 'Yes Attrition' class but a lower precision, meaning it identifies more employees who leave but also has more false positives."
      ],
      "metadata": {
        "id": "a0eUvM2KKk0b"
      }
    }
  ]
}