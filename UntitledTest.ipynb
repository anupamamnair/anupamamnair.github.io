{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anupamamnair/anupamamnair.github.io/blob/master/UntitledTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYuZnI2r5nEF",
        "outputId": "a208bc26-0517-499c-e75f-7076463f0f6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torchtext) (4.67.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchtext) (2.32.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchtext) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext) (2026.1.4)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchtext scikit-learn\n",
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import sys\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec, FastText"
      ],
      "metadata": {
        "id": "IKNFEepG5s1Z"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = fetch_20newsgroups(\n",
        "    subset='train',\n",
        "    remove=('headers', 'footers', 'quotes')\n",
        ")\n",
        "\n",
        "sample_index = 0\n",
        "sample_document = train_data.data[sample_index]\n",
        "sample_label = train_data.target[sample_index]\n",
        "sample_category_name = train_data.target_names[sample_label]\n",
        "\n",
        "print(\"\\n--- Sample Document ---\")\n",
        "print(f\"\\nDocument: {sample_document[:500]}...\")\n",
        "print(f\"\\nNumeric Label: {sample_label}\")\n",
        "print(f\"\\nCategory Name: {sample_category_name}\")\n",
        "\n",
        "test_data = fetch_20newsgroups(\n",
        "    subset='test',\n",
        "    remove=('headers', 'footers', 'quotes')\n",
        ")\n",
        "\n",
        "X_train_text = train_data.data\n",
        "y_train = train_data.target\n",
        "\n",
        "X_test_text = test_data.data\n",
        "y_test = test_data.target"
      ],
      "metadata": {
        "id": "w57iyJmz75nm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72e66d7a-164b-4e75-99b4-837f3bb7b7f3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sample Document ---\n",
            "\n",
            "Document: I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I know. If anyone can tellme a model name, engine specs, years\n",
            "of production, where this car is made, history, or whatever info you\n",
            "have on this funky looking car, please e-mail....\n",
            "\n",
            "Numeric Label: 7\n",
            "\n",
            "Category Name: rec.autos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = set(ENGLISH_STOP_WORDS)\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
        "    tokens = text.split()\n",
        "    tokens = [w for w in tokens if w not in stopwords and len(w) > 2]\n",
        "    return tokens\n",
        "\n",
        "\n",
        "# def preprocess(text):\n",
        "#     text = text.lower()\n",
        "#     text = re.sub(r'[^a-z\\s]', ' ', text)\n",
        "#     tokens = text.split()\n",
        "#     tokens = [w for w in tokens if w not in stopwords]\n",
        "#     return tokens\n",
        "\n",
        "# Adding this common method to ensure fair comparison between models\n",
        "\n",
        "train_tokens = [preprocess(t) for t in X_train_text]\n",
        "test_tokens  = [preprocess(t) for t in X_test_text]"
      ],
      "metadata": {
        "id": "56gYYfkmki4v"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Creating features for TF-IDF\")\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def identity(x):\n",
        "    return x\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    tokenizer=identity,\n",
        "    preprocessor=identity,\n",
        "    token_pattern=None,\n",
        "    lowercase=False,\n",
        "    max_features=20000\n",
        ")\n",
        "\n",
        "X_train_tfidf = vectorizer.fit_transform(train_tokens).toarray()\n",
        "X_test_tfidf  = vectorizer.transform(test_tokens).toarray()"
      ],
      "metadata": {
        "id": "HFYbcwIg8Hr2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "350444a6-4656-4b36-b258-c48eda02076c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating features for TF-IDF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_temp = TfidfVectorizer(tokenizer=lambda x: x, lowercase=False)\n",
        "tfidf_temp.fit(train_tokens)\n",
        "\n",
        "idf_dict = dict(zip(tfidf_temp.get_feature_names_out(), tfidf_temp.idf_))\n",
        "\n",
        "\n",
        "def avg_vec(tokens, model, dim):\n",
        "    vecs = []\n",
        "    weights = []\n",
        "\n",
        "    for w in tokens:\n",
        "        if w in model:\n",
        "            vecs.append(model[w])\n",
        "            weights.append(idf_dict.get(w, 1.0))  # IDF weight\n",
        "\n",
        "    if len(vecs) == 0:\n",
        "        return np.zeros(dim)\n",
        "\n",
        "    vecs = np.array(vecs)\n",
        "    weights = np.array(weights).reshape(-1, 1)\n",
        "\n",
        "    return np.sum(vecs * weights, axis=0) / np.sum(weights)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzTR3L51xaOC",
        "outputId": "c270a5d3-80dd-461b-9d2b-51146c76d9e3"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Creating features for Word2Vec\")\n",
        "\n",
        "dim = 300\n",
        "w2v = Word2Vec(train_tokens, vector_size=dim, window=5, min_count=2)\n",
        "\n",
        "X_train_w2v = np.array([avg_vec(t, w2v.wv, dim) for t in train_tokens])\n",
        "X_test_w2v  = np.array([avg_vec(t, w2v.wv, dim) for t in test_tokens])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pRjKFMlmatt",
        "outputId": "25901e3e-94ea-4661-994a-8eb2b9419c12"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating features for Word2Vec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading GloVe embeddings...\")\n",
        "\n",
        "glove = api.load(\"glove-wiki-gigaword-300\")\n",
        "\n",
        "X_train_glove = np.array([avg_vec(t, glove, dim) for t in train_tokens])\n",
        "X_test_glove  = np.array([avg_vec(t, glove, dim) for t in test_tokens])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOloDvCNmda5",
        "outputId": "58e1b1a4-e8ad-47da-9793-06e35bc6b045"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading GloVe embeddings...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Creating features for FastText\")\n",
        "\n",
        "ft = FastText(train_tokens, vector_size=dim, window=5, min_count=2)\n",
        "\n",
        "X_train_ft = np.array([avg_vec(t, ft.wv, dim) for t in train_tokens])\n",
        "X_test_ft  = np.array([avg_vec(t, ft.wv, dim) for t in test_tokens])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxu7EHUnmeBb",
        "outputId": "acd1de8f-4ac2-48d0-a622-cf785192d662"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating features for FastText\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FFNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ],
      "metadata": {
        "id": "XSkkRR4h82lD"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding existing code to a method to reuse the neural network training and evaluation\n",
        "\n",
        "def run_model(X_train_vec, X_test_vec, name):\n",
        "  print(f\"\\nRunning {name}...\")\n",
        "  print(\"======================================\")\n",
        "  print(\"Training model...\")\n",
        "\n",
        "  X_train_run = torch.tensor(X_train_vec, dtype=torch.float32)\n",
        "  y_train_run = torch.tensor(y_train, dtype=torch.long)\n",
        "\n",
        "  X_test_run = torch.tensor(X_test_vec, dtype=torch.float32)\n",
        "  y_test_run = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "  train_dataset = TensorDataset(X_train_run, y_train_run)\n",
        "  test_dataset  = TensorDataset(X_test_run, y_test_run)\n",
        "\n",
        "  train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "  test_loader  = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "  input_dim = X_train_vec.shape[1]\n",
        "  hidden_dim = 256\n",
        "  num_classes = len(set(y_train))\n",
        "\n",
        "  model = FFNN(input_dim, hidden_dim, num_classes)\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "  epochs = 5\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for xb, yb in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(xb)\n",
        "        loss = criterion(logits, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n",
        "\n",
        "  model.eval()\n",
        "  all_preds = []\n",
        "  all_true = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for xb, yb in test_loader:\n",
        "      logits = model(xb)\n",
        "      preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "      all_preds.extend(preds.cpu().numpy())\n",
        "      all_true.extend(yb.cpu().numpy())\n",
        "\n",
        "  acc = accuracy_score(all_true, all_preds)\n",
        "  prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "      all_true, all_preds, average='weighted'\n",
        "  )\n",
        "\n",
        "  print(\"Results : \")\n",
        "  print(f\"{name} -> Acc:{acc:.4f}  F1:{f1:.4f}\")\n",
        "\n",
        "  return [name, acc, prec, rec, f1]"
      ],
      "metadata": {
        "id": "ihYiUTnImomh"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "results.append(run_model(X_train_tfidf, X_test_tfidf, \"TF-IDF\"))\n",
        "results.append(run_model(X_train_w2v, X_test_w2v, \"Word2Vec\"))\n",
        "results.append(run_model(X_train_glove, X_test_glove, \"GloVe\"))\n",
        "results.append(run_model(X_train_ft, X_test_ft, \"FastText\"))\n",
        "\n",
        "df = pd.DataFrame(\n",
        "    results,\n",
        "    columns=[\"Representation\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"]\n",
        ")\n",
        "\n",
        "print(\"\\n========== FINAL COMPARISON ==========\")\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9690u1NxN8X",
        "outputId": "0b85e535-8812-4ee9-ee53-70840578e517"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running TF-IDF...\n",
            "======================================\n",
            "Training model...\n"
          ]
        }
      ]
    }
  ]
}